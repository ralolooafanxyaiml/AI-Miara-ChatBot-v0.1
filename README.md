# AI-Miara-ChatBot-v0.1

# ğŸ§  BabyLLM: Transformer from Scratch

### ğŸš€ Overview
This project implements a **Generative Pre-trained Transformer (GPT)** architecture from scratch using **TensorFlow/Keras**. Unlike standard wrappers, this codebase defines the **Multi-Head Attention** mechanism, **Positional Embeddings**, and **Encoder Layers** manually to train a language model on custom text data.

### ğŸ› ï¸ Quick Start

1. **Setup Data:**
   Create a folder named `raw_data/` and add any `.txt` file (e.g., movie dialogs, books).

2. **Train the Brain:**
   ```bash
   python train_model.py
